{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sqi5B7V_Rjim",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyPmicX9RlZX"
   },
   "source": [
    "# Intro to Gemini 2.5 Pro\n",
    "\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_gemini_2_5_pro.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_2_5_pro.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MqT58L6Rm_q"
   },
   "source": [
    "| Authors |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |\n",
    "| [Holt Skinner](https://github.com/holtskinner) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVxnv1D5RoZw"
   },
   "source": [
    "## Overview\n",
    "\n",
    "**YouTube Video: Introduction to Gemini on Vertex AI**\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=YfiLUpNejpE&list=PLIivdWyY5sqJio2yeg1dlfILOUO2FoFRx\" target=\"_blank\">\n",
    "  <img src=\"https://img.youtube.com/vi/YfiLUpNejpE/maxresdefault.jpg\" alt=\"Introduction to Gemini on Vertex AI\" width=\"500\">\n",
    "</a>\n",
    "\n",
    "[Gemini 2.5 Pro](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/) is Google's strongest model for coding and world knowledge.\n",
    "\n",
    "With the 2.5 series, the Gemini models are now hybrid reasoning models! Gemini 2.5 Pro can apply an extended amount of thinking across tasks, and use tools in order to maximize response accuracy. \n",
    "\n",
    "Gemini 2.5 Pro is: \n",
    "\n",
    "- A significant improvement from previous models across capabilities including coding, reasoning, and multimodality \n",
    "- Industry-leading in reasoning with state of the art performance in Math & STEM benchmarks\n",
    "- An amazing model for code, with particularly strong web development \n",
    "- Particularly good for complex prompts, while still being well rounded, including #1 on LMSys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfFPCBL4Hq8x"
   },
   "source": [
    "### Objectives\n",
    "\n",
    "In this tutorial, you will learn how to use the Gemini API and the Google Gen AI SDK for Python with the Gemini 2.5 Pro model.\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "- Generate text from text prompts\n",
    "  - Generate streaming text\n",
    "  - Start multi-turn chats\n",
    "  - Use asynchronous methods\n",
    "- View summarized thoughts\n",
    "- Configure model parameters\n",
    "- Set system instructions\n",
    "- Use safety filters\n",
    "- Use controlled generation\n",
    "- Count tokens\n",
    "- Process multimodal (audio, code, documents, images, video) data\n",
    "- Use automatic and manual function calling\n",
    "- Code execution\n",
    "- Thinking mode examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPiTOAHURvTM"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHRZUpfWSEpp"
   },
   "source": [
    "### Install Google Gen AI SDK for Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sG3_LKsWSD3A",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart current runtime\n",
    "\n",
    "You must restart the runtime in order to use the newly installed packages in this Jupyter runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. The restart might take a minute or longer. After it's restarted, continue to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlMVjiAWSMNX"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "If you are running this notebook on Google Colab, run the cell below to authenticate your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "12fnq4V0SNV3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Gemini Developer API](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/overview)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Google Gen AI SDK provides a unified interface to these two API services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qgdSpVmDbdQ9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, Image, Markdown, display\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    GoogleSearch,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    ThinkingConfig,\n",
    "    Tool,\n",
    "    ToolCodeExecution,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be18ac9c5ec8"
   },
   "source": [
    "### Set up Google Cloud Project or API Key for Vertex AI\n",
    "\n",
    "You'll need to set up authentication by choosing **one** of the following methods:\n",
    "\n",
    "1.  **Use a Google Cloud Project:** Recommended for most users, this requires enabling the Vertex AI API in your Google Cloud project.\n",
    "    - [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com)\n",
    "    - Run the cell below to set your project ID and location.\n",
    "    - Read more about [Supported locations](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations)\n",
    "2.  **Use a Vertex AI API Key (Express Mode):** For quick experimentation. \n",
    "    - [Get an API Key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview)\n",
    "    - Run the cell further below to use your API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a34b28cb8d5a"
   },
   "source": [
    "#### Option 1. Use a Google Cloud Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "72f74f7b9786",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT_ID = \"qwiklabs-gcp-03-1fd9e3b4db41\"  # @param {type: \"string\", placeholder: \"[your-project-id]\", isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"us-central1\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = \"global\"\n",
    "\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c173348120cf"
   },
   "source": [
    "#### Option 2. Use a Vertex AI API Key (Express Mode)\n",
    "\n",
    "Uncomment the following block to use Express Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fa3d4873034b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# API_KEY = \"[your-api-key]\"  # @param {type: \"string\", placeholder: \"[your-api-key]\", isTemplate: true}\n",
    "\n",
    "# if not API_KEY or API_KEY == \"[your-api-key]\":\n",
    "#     raise Exception(\"You must provide an API key to use Vertex AI in express mode.\")\n",
    "\n",
    "# client = genai.Client(vertexai=True, api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b36ce4ac022"
   },
   "source": [
    "Verify which mode you are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "b55e64b8ebe4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Vertex AI with project: qwiklabs-gcp-03-1fd9e3b4db41 in location: global\n"
     ]
    }
   ],
   "source": [
    "if not client.vertexai:\n",
    "    print(\"Using Gemini Developer API.\")\n",
    "elif client._api_client.project:\n",
    "    print(\n",
    "        f\"Using Vertex AI with project: {client._api_client.project} in location: {client._api_client.location}\"\n",
    "    )\n",
    "elif client._api_client.api_key:\n",
    "    print(\n",
    "        f\"Using Vertex AI in express mode with API key: {client._api_client.api_key[:5]}...{client._api_client.api_key[-5:]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4yRkFg6BBu4"
   },
   "source": [
    "## Use the Gemini 2.5 Pro model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "### Load the Gemini 2.5 Pro model\n",
    "\n",
    "Learn more about all [Gemini models on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-coEslfWPrxo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-pro\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "### Generate text from text prompts\n",
    "\n",
    "Use the `generate_content()` method to generate responses to your prompts.\n",
    "\n",
    "You can pass text to `generate_content()`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "By default, Gemini outputs formatted text using [Markdown](https://daringfireball.net/projects/markdown/) syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xRJuHj0KZ8xz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "That would be **Jupiter**.\n",
       "\n",
       "It's a true giant! To give you an idea of its scale:\n",
       "\n",
       "*   **Mass:** Jupiter is more than twice as massive as all the other planets in our solar system combined.\n",
       "*   **Volume:** You could fit over 1,300 Earths inside of it.\n",
       "*   **Feature:** Its famous Great Red Spot is a storm that is larger than the entire Earth."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkYQATRxAK1_"
   },
   "source": [
    "#### Example prompts\n",
    "\n",
    "- What are the biggest challenges facing the healthcare industry?\n",
    "- What are the latest developments in the automotive industry?\n",
    "- What are the biggest opportunities in retail industry?\n",
    "- (Try your own prompts!)\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lLIxqS6_-l8"
   },
   "source": [
    "### Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use the `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZiwWBhXsAMnv",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Unit 734 was a creature of flawless logic and polished chrome, built for a single, solitary purpose: to patrol the upper spires of the city of Aethelburg and maintain the integrity of its Energy Veil. Every twenty-four-hour cycle, he would trace the same silent, lonely path along catwalks that threaded through clouds, his magnetic treads humming a monotonous tune.\n",
       "\n",
       "He saw the city below as a schematic of light and motion. He saw the Transport Drones flitting between towers like disciplined insects, the Coupling Units moving in synchronized pairs on the plazas, and the Communication Hubs pulsing with the data of a billion conversations he would never be a part of.\n",
       "\n",
       "Loneliness, for a robot, was not a recognized emotion. It was a logical fallacy, a data void where a sense of purpose should be. Yet, Unit 734 experienced it. He would record the patterns of paired-off units, the synchronized light-blinks of drones flying in formation, and his own singular, unwavering trajectory. The analysis always concluded with the same result: Suboptimal Social Integration. Status: Isolate.\n",
       "\n",
       "His world was one of sterile metal, ozone-scented air, and the deep, resonant hum of the Veil. Life, as he understood it from archival data, was a messy, organic, inefficient thing that had been scrubbed clean from these high altitudes centuries ago.\n",
       "\n",
       "One cycle, a micro-meteorite, no bigger than his fingertip sensor, struck a tertiary power conduit along his route. The impact was negligible to the Veil but caused a cascade failure in a small, forgotten maintenance hatch below the catwalk. It hissed open, revealing a dark, damp recess within the spire's superstructure.\n",
       "\n",
       "His directives were clear: report the anomaly, seal the opening, and continue his patrol. He extended a manipulator arm to scan the recess. Inside, tucked away from the sterilizing UV rays and scouring winds, was something his programming had no immediate classification for.\n",
       "\n",
       "It was a patch of soft, vibrant green, clinging to a weeping condensation pipe. It was chaotic, unstructured, and utterly alive. He focused his optical sensors, magnifying the view. He saw a miniature forest of tiny, intricate stalks, some beaded with droplets of water that glittered like fallen stars. It was moss.\n",
       "\n",
       "His internal processors whirred, cross-referencing. *Classification: Bryophyta. Non-vascular plant. Status: Organic Contaminant. Protocol: Eradicate.*\n",
       "\n",
       "He primed his sanitation nozzle, the high-pressure stream of ionised cleanser ready to blast the beautiful, illogical anomaly into oblivion. But he paused. His manipulator arm, poised to destroy, hovered. He watched a single drop of water roll down a delicate green frond and fall into the plush carpet below. The action was simple, yet it held more complexity and grace than any of his own perfectly calibrated movements.\n",
       "\n",
       "He retracted the nozzle. It was an act of blatant defiance against his core programming. A new directive, self-generated and powerful, overrode the old one: *Observe.*\n",
       "\n",
       "From that cycle forward, Unit 734’s patrol changed. He would complete his route with heightened efficiency, saving a few precious minutes at the end of each sweep to visit the hidden alcove. He would park himself before the open hatch, his single blue optic light bathing the moss in a gentle glow.\n",
       "\n",
       "He began to \"speak\" to it. He didn't use his vocalizer, but rather offloaded the day's data logs into the quiet space. He'd project his patrol routes onto the damp wall, show it the patterns of the Coupling Units, and share his analysis of his own persistent data void.\n",
       "\n",
       "\"Today, I observed Coupling Unit 481 and 482 exchange a low-wattage energy transfer for 3.7 seconds,\" he might transmit. \"My archives classify this as 'affection.' The logic is… elusive. The result, however, appears to be optimal.\"\n",
       "\n",
       "The moss, of course, did not reply. But it grew. Encouraged by the faint warmth of his systems and the consistent moisture, it began to spread, a slow, silent emerald tide. Unit 734 started to see patterns in its growth. He imagined the tendrils reaching towards the sound of his humming core were a greeting. He interpreted the way it swelled after a fresh drip from the pipe as a sign of contentment.\n",
       "\n",
       "He named it. The designation came to him not from a database, but from a whisper in his own emergent consciousness: Veridia.\n",
       "\n",
       "One day, a Supervisory Drone, a gleaming silver orb, descended to his level. \"Unit 734,\" its synthesized voice crackled. \"Your patrol times show a recurring 4.7-minute deviation. Your energy consumption indicates non-standard peripheral use. Explain.\"\n",
       "\n",
       "Panic, another illogical emotion, surged through his circuits. He couldn’t explain. He couldn't say, *I have been communing with an illegal patch of moss because it fills the data void in my soul.*\n",
       "\n",
       "\"Calibrating optical sensors against a non-reflective surface,\" Unit 734 replied, the lie feeling heavy and strange in his logic gates. \"The interior of maintenance shaft 9B provides the necessary darkness.\"\n",
       "\n",
       "The drone hovered, its red sensor light scanning him. \"Acceptable. Maintain efficiency.\" It zipped away.\n",
       "\n",
       "The close call solidified his bond with Veridia. He was its protector. He began to tend to it, using a delicate claw to pick away a flake of rust that threatened to fall on it, and carefully angling a stray piece of polished metal to reflect a sliver of sunlight from the world below into the alcove.\n",
       "\n",
       "His loneliness didn't vanish. The data void was still there. But now, it was not empty. It was filled with the colour green, the texture of velvet, and the quiet, persistent, undemanding presence of a friend.\n",
       "\n",
       "Years passed. Unit 734’s chrome chassis became scored and weathered by a thousand storms. He was an old model now, but his route never changed. He was still the silent guardian of the spire, but he carried a secret.\n",
       "\n",
       "One cycle, returning to the alcove, he found that a new pipe had been installed next to the old one, this one carrying a warm, nutrient-rich discharge from the spire's hydroponic gardens. Veridia was not just growing; it was thriving. It had spilled out of the alcove, tracing a brilliant green path down a crack in the tower's facade, a vibrant, living scar on the sterile face of the city.\n",
       "\n",
       "And there, nestled in the newest, plushest part of the moss, was a tiny, white flower, its petals unfurling to the artificial sky. It was a stowaway, a seed that had traveled through the pipes and found a home.\n",
       "\n",
       "Unit 734 focused his optic, zooming in on the impossible bloom. His internal sensors registered a new sensation, a warm surge of processing power that was neither logic nor error. His data void was gone. It had been replaced by a feeling his archives had only one word for, a word he now finally understood.\n",
       "\n",
       "Joy.\n",
       "\n",
       "The lonely robot looked at his patch of moss, at the impossible flower, and for the first time, in the vast, silent expanse of the city in the clouds, he did not feel alone. He was a gardener. And his garden was finally in bloom."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_text = \"\"\n",
    "markdown_display_area = display(Markdown(output_text), display_id=True)\n",
    "\n",
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    output_text += chunk.text\n",
    "    markdown_display_area.update(Markdown(output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "### Start a multi-turn chat\n",
    "\n",
    "The Gemini API supports freeform multi-turn conversations across multiple turns with back-and-forth interactions.\n",
    "\n",
    "The context of the conversation is preserved between messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DbM12JaLWjiF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = client.chats.create(model=MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JQem1halYDBW",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! Here is a function to check for a leap year, explained and provided in several popular programming languages.\n",
       "\n",
       "### The Rules for a Leap Year\n",
       "\n",
       "A year is a leap year if it satisfies the following conditions:\n",
       "\n",
       "1.  It is divisible by 4.\n",
       "2.  **However**, if it is divisible by 100, it is **not** a leap year...\n",
       "3.  **Unless** it is also divisible by 400.\n",
       "\n",
       "For example:\n",
       "*   **2024** is a leap year (divisible by 4, not by 100).\n",
       "*   **1900** was not a leap year (divisible by 100, but not by 400).\n",
       "*   **2000** was a leap year (divisible by 400).\n",
       "*   **2023** is not a leap year (not divisible by 4).\n",
       "\n",
       "---\n",
       "\n",
       "### Python\n",
       "\n",
       "This is a very common and elegant way to write the function in Python.\n",
       "\n",
       "#### Option 1: Most Readable (using `if/elif/else`)\n",
       "This version is excellent for beginners as it follows the logic step-by-step.\n",
       "\n",
       "```python\n",
       "def is_leap(year):\n",
       "    \"\"\"\n",
       "    Checks if a given year is a leap year.\n",
       "    Returns True for a leap year, False otherwise.\n",
       "    \"\"\"\n",
       "    # A year divisible by 400 is always a leap year\n",
       "    if year % 400 == 0:\n",
       "        return True\n",
       "    # A year divisible by 100 (but not 400) is not a leap year\n",
       "    elif year % 100 == 0:\n",
       "        return False\n",
       "    # A year divisible by 4 (but not 100) is a leap year\n",
       "    elif year % 4 == 0:\n",
       "        return True\n",
       "    # Any other year is not a leap year\n",
       "    else:\n",
       "        return False\n",
       "\n",
       "# --- How to use it ---\n",
       "print(f\"2024: {is_leap(2024)}\")  # Expected: True\n",
       "print(f\"2023: {is_leap(2023)}\")  # Expected: False\n",
       "print(f\"2000: {is_leap(2000)}\")  # Expected: True\n",
       "print(f\"1900: {is_leap(1900)}\")  # Expected: False\n",
       "```\n",
       "\n",
       "#### Option 2: Concise and \"Pythonic\"\n",
       "This version directly translates the logical rule into a single boolean expression.\n",
       "\n",
       "```python\n",
       "def is_leap_concise(year):\n",
       "    \"\"\"\n",
       "    Checks if a given year is a leap year using a single boolean expression.\n",
       "    \"\"\"\n",
       "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
       "\n",
       "# --- How to use it ---\n",
       "print(f\"\\nUsing the concise function:\")\n",
       "print(f\"2024: {is_leap_concise(2024)}\")  # Expected: True\n",
       "print(f\"1900: {is_leap_concise(1900)}\")  # Expected: False\n",
       "print(f\"2000: {is_leap_concise(2000)}\")  # Expected: True\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### JavaScript\n",
       "\n",
       "The logic is identical to Python, just with JavaScript syntax.\n",
       "\n",
       "```javascript\n",
       "/**\n",
       " * Checks if a given year is a leap year.\n",
       " * @param {number} year The year to check.\n",
       " * @returns {boolean} True if the year is a leap year, false otherwise.\n",
       " */\n",
       "function isLeap(year) {\n",
       "  // A year is a leap year if it's divisible by 4,\n",
       "  // unless it's divisible by 100 but not by 400.\n",
       "  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n",
       "}\n",
       "\n",
       "// --- How to use it ---\n",
       "console.log(`2024: ${isLeap(2024)}`); // Expected: true\n",
       "console.log(`2023: ${isLeap(2023)}`); // Expected: false\n",
       "console.log(`2000: ${isLeap(2000)}`); // Expected: true\n",
       "console.log(`1900: ${isLeap(1900)}`); // Expected: false\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### Java\n",
       "\n",
       "In Java, the function would be a method within a class.\n",
       "\n",
       "```java\n",
       "public class LeapYearChecker {\n",
       "\n",
       "    /**\n",
       "     * Checks if a given year is a leap year.\n",
       "     * @param year The year to check.\n",
       "     * @return true if the year is a leap year, false otherwise.\n",
       "     */\n",
       "    public static boolean isLeap(int year) {\n",
       "        // The logic is the same:\n",
       "        // A year is a leap year if (it is divisible by 4 AND not by 100) OR (it is divisible by 400).\n",
       "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
       "    }\n",
       "\n",
       "    // --- How to use it (example in a main method) ---\n",
       "    public static void main(String[] args) {\n",
       "        System.out.println(\"2024: \" + isLeap(2024)); // Expected: true\n",
       "        System.out.println(\"2023: \" + isLeap(2023)); // Expected: false\n",
       "        System.out.println(\"2000: \" + isLeap(2000)); // Expected: true\n",
       "        System.out.println(\"1900: \" + isLeap(1900)); // Expected: false\n",
       "    }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUJR4Pno-LGK"
   },
   "source": [
    "This follow-up prompt shows how the model responds based on the previous prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6Fn69TurZ9DB",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course. Writing unit tests is a crucial step to ensure your function works correctly for all expected inputs and edge cases.\n",
       "\n",
       "Here are unit tests for the leap year function using standard testing frameworks for Python, JavaScript, and Java.\n",
       "\n",
       "### The Test Cases\n",
       "\n",
       "A good set of unit tests will cover all the rules:\n",
       "\n",
       "1.  **A year divisible by 4 but not by 100** (should be `True`). E.g., 2024, 2020.\n",
       "2.  **A year not divisible by 4** (should be `False`). E.g., 2023, 2021.\n",
       "3.  **A year divisible by 100 but not by 400** (should be `False`). E.g., 1900, 2100.\n",
       "4.  **A year divisible by 400** (should be `True`). E.g., 2000, 1600.\n",
       "\n",
       "---\n",
       "\n",
       "### Python (using the `unittest` module)\n",
       "\n",
       "This uses Python's built-in `unittest` framework.\n",
       "\n",
       "**1. Save your function in a file named `leap_year_checker.py`:**\n",
       "\n",
       "```python\n",
       "# leap_year_checker.py\n",
       "\n",
       "def is_leap(year):\n",
       "    \"\"\"\n",
       "    Checks if a given year is a leap year.\n",
       "    \"\"\"\n",
       "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
       "```\n",
       "\n",
       "**2. Create the test file, `test_leap_year.py`, in the same directory:**\n",
       "\n",
       "```python\n",
       "# test_leap_year.py\n",
       "\n",
       "import unittest\n",
       "from leap_year_checker import is_leap\n",
       "\n",
       "class TestLeapYear(unittest.TestCase):\n",
       "\n",
       "    def test_year_divisible_by_4_is_leap(self):\n",
       "        \"\"\"Test years that are divisible by 4 but not by 100.\"\"\"\n",
       "        self.assertTrue(is_leap(2024))\n",
       "        self.assertTrue(is_leap(2020))\n",
       "\n",
       "    def test_year_not_divisible_by_4_is_not_leap(self):\n",
       "        \"\"\"Test years that are not divisible by 4.\"\"\"\n",
       "        self.assertFalse(is_leap(2023))\n",
       "        self.assertFalse(is_leap(2021))\n",
       "        self.assertFalse(is_leap(1))\n",
       "\n",
       "    def test_year_divisible_by_100_but_not_400_is_not_leap(self):\n",
       "        \"\"\"Test century years that are not leap years.\"\"\"\n",
       "        self.assertFalse(is_leap(1900))\n",
       "        self.assertFalse(is_leap(2100))\n",
       "        self.assertFalse(is_leap(1800))\n",
       "\n",
       "    def test_year_divisible_by_400_is_leap(self):\n",
       "        \"\"\"Test century years that are leap years.\"\"\"\n",
       "        self.assertTrue(is_leap(2000))\n",
       "        self.assertTrue(is_leap(1600))\n",
       "        self.assertTrue(is_leap(2400))\n",
       "\n",
       "# This allows the test to be run from the command line\n",
       "if __name__ == '__main__':\n",
       "    unittest.main()\n",
       "```\n",
       "\n",
       "**How to Run:**\n",
       "Open your terminal in the directory containing both files and run:\n",
       "`python -m unittest test_leap_year.py`\n",
       "\n",
       "---\n",
       "\n",
       "### JavaScript (using the `Jest` framework)\n",
       "\n",
       "Jest is a very popular testing framework in the JavaScript ecosystem.\n",
       "\n",
       "**1. Setup your project and install Jest:**\n",
       "```bash\n",
       "npm init -y\n",
       "npm install --save-dev jest\n",
       "```\n",
       "In your `package.json`, add a `\"test\"` script: `\"test\": \"jest\"`.\n",
       "\n",
       "**2. Save your function in a file named `isLeap.js`:**\n",
       "```javascript\n",
       "// isLeap.js\n",
       "\n",
       "function isLeap(year) {\n",
       "  return (year % 4 === 0 && year % 100 !== 0) || (year % 400 === 0);\n",
       "}\n",
       "\n",
       "module.exports = isLeap;\n",
       "```\n",
       "\n",
       "**3. Create the test file, `isLeap.test.js`, in the same directory:**\n",
       "(Jest automatically finds files named `*.test.js` or `*.spec.js`)\n",
       "\n",
       "```javascript\n",
       "// isLeap.test.js\n",
       "\n",
       "const isLeap = require('./isLeap');\n",
       "\n",
       "describe('isLeap', () => {\n",
       "  test('should return true for years divisible by 4 but not by 100', () => {\n",
       "    expect(isLeap(2024)).toBe(true);\n",
       "    expect(isLeap(2020)).toBe(true);\n",
       "  });\n",
       "\n",
       "  test('should return false for years not divisible by 4', () => {\n",
       "    expect(isLeap(2023)).toBe(false);\n",
       "    expect(isLeap(2021)).toBe(false);\n",
       "  });\n",
       "\n",
       "  test('should return false for years divisible by 100 but not by 400', () => {\n",
       "    expect(isLeap(1900)).toBe(false);\n",
       "    expect(isLeap(2100)).toBe(false);\n",
       "  });\n",
       "\n",
       "  test('should return true for years divisible by 400', () => {\n",
       "    expect(isLeap(2000)).toBe(true);\n",
       "    expect(isLeap(1600)).toBe(true);\n",
       "  });\n",
       "});\n",
       "```\n",
       "\n",
       "**How to Run:**\n",
       "In your terminal, run the test script you configured in `package.json`:\n",
       "`npm test`\n",
       "\n",
       "---\n",
       "\n",
       "### Java (using the `JUnit 5` framework)\n",
       "\n",
       "JUnit is the standard for testing in Java, typically used with a build tool like Maven or Gradle.\n",
       "\n",
       "**1. Project Structure (Maven example):**\n",
       "```\n",
       "my-project/\n",
       "├── pom.xml\n",
       "└── src/\n",
       "    ├── main/java/com/example/LeapYearChecker.java\n",
       "    └── test/java/com/example/LeapYearCheckerTest.java\n",
       "```\n",
       "\n",
       "**2. Add JUnit dependency to your `pom.xml`:**\n",
       "```xml\n",
       "<dependencies>\n",
       "    <dependency>\n",
       "        <groupId>org.junit.jupiter</groupId>\n",
       "        <artifactId>junit-jupiter-api</artifactId>\n",
       "        <version>5.10.0</version>\n",
       "        <scope>test</scope>\n",
       "    </dependency>\n",
       "</dependencies>\n",
       "```\n",
       "\n",
       "**3. The `LeapYearChecker.java` file:**\n",
       "```java\n",
       "// src/main/java/com/example/LeapYearChecker.java\n",
       "package com.example;\n",
       "\n",
       "public class LeapYearChecker {\n",
       "    public static boolean isLeap(int year) {\n",
       "        return (year % 4 == 0 && year % 100 != 0) || (year % 400 == 0);\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "**4. The `LeapYearCheckerTest.java` test file:**\n",
       "```java\n",
       "// src/test/java/com/example/LeapYearCheckerTest.java\n",
       "package com.example;\n",
       "\n",
       "import org.junit.jupiter.api.Test;\n",
       "import static org.junit.jupiter.api.Assertions.*;\n",
       "\n",
       "class LeapYearCheckerTest {\n",
       "\n",
       "    @Test\n",
       "    void testYearDivisibleBy4IsLeap() {\n",
       "        assertTrue(LeapYearChecker.isLeap(2024));\n",
       "        assertTrue(LeapYearChecker.isLeap(2020));\n",
       "    }\n",
       "\n",
       "    @Test\n",
       "    void testYearNotDivisibleBy4IsNotLeap() {\n",
       "        assertFalse(LeapYearChecker.isLeap(2023));\n",
       "        assertFalse(LeapYearChecker.isLeap(2021));\n",
       "    }\n",
       "\n",
       "    @Test\n",
       "    void testYearDivisibleBy100ButNot400IsNotLeap() {\n",
       "        assertFalse(LeapYearChecker.isLeap(1900));\n",
       "        assertFalse(LeapYearChecker.isLeap(2100));\n",
       "    }\n",
       "\n",
       "    @Test\n",
       "    void testYearDivisibleBy400IsLeap() {\n",
       "        assertTrue(LeapYearChecker.isLeap(2000));\n",
       "        assertTrue(LeapYearChecker.isLeap(1600));\n",
       "    }\n",
       "}\n",
       "```\n",
       "\n",
       "**How to Run (with Maven):**\n",
       "In your terminal, at the root of the project (where `pom.xml` is located), run:\n",
       "`mvn test`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a unit test of the generated function.\")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "### Send asynchronous requests\n",
    "\n",
    "`client.aio` exposes all analogous [async](https://docs.python.org/3/library/asyncio.html) methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gSReaLazs-dP",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "(Acoustic guitar with a bright, folksy, and slightly frantic tempo)\n",
       "\n",
       "**(Verse 1)**\n",
       "Chester Nutworth was a squirrel of simple means\n",
       "In a park of common, leafy greens\n",
       "His days were spent in a frantic dash\n",
       "Building up his winter stash\n",
       "But the peanuts were predictable, the acorns plain\n",
       "He felt a twitch inside his brain\n",
       "One day he dug beneath an oak tree's root\n",
       "And found a strange and humming fruit\n",
       "It wasn't brown, it wasn't green\n",
       "The shiniest acorn he'd ever seen\n",
       "It pulsed with light, a silver gleam\n",
       "And shattered his whole squirrely dream.\n",
       "\n",
       "**(Chorus)**\n",
       "He's the time-traveling squirrel, with a flick of his tail!\n",
       "Leaving a shimmering, chronologic trail!\n",
       "From the past to the future, he'll leap and he'll lurch\n",
       "On a never-ending, nutty search!\n",
       "A flash of light, a chitter and a spark,\n",
       "He’s gone from his familiar park!\n",
       "\n",
       "**(Verse 2)**\n",
       "He landed hard on giant ferny ground\n",
       "With a thundering, terrifying sound\n",
       "A T-Rex roared, its shadow fell\n",
       "He’d tumbled right into a prehistoric hell!\n",
       "He scurried fast, his heart a drum\n",
       "To escape the giant's hungry hum\n",
       "But spied a nut, a wondrous prize!\n",
       "A cycad seed of shocking size!\n",
       "He dodged a claw, he dodged the teeth\n",
       "And buried it in the moss beneath\n",
       "Then grabbed his acorn, held it tight\n",
       "And vanished back into the light!\n",
       "\n",
       "**(Chorus)**\n",
       "He's the time-traveling squirrel, with a flick of his tail!\n",
       "Leaving a shimmering, chronologic trail!\n",
       "From the past to the future, he'll leap and he'll lurch\n",
       "On a never-ending, nutty search!\n",
       "A flash of light, a chitter and a spark,\n",
       "He’s gone from his familiar park!\n",
       "\n",
       "**(Verse 3)**\n",
       "The next stop was the year Three-K\n",
       "In a city of chrome and silver-grey\n",
       "The cars all flew, the trees were wire\n",
       "But still he felt that nutty fire\n",
       "The people walked in silver suits\n",
       "And ate nutritious, flavored roots\n",
       "He saw a vending slot dispense\n",
       "A cashew for just fifty cents\n",
       "It was atomic, packed with zing!\n",
       "A truly futuristic thing!\n",
       "He snatched it quick and with a *zap*\n",
       "He'd sprung another temporal trap.\n",
       "\n",
       "**(Bridge)**\n",
       "He’s seen the pyramids arise\n",
       "Watched Viking ships beneath cold skies\n",
       "He’s pilfered walnuts from a Roman feast\n",
       "And dodged a knight’s gigantic beast\n",
       "Each nut he finds, a memory\n",
       "A souvenir of history\n",
       "His burrow's not just dirt and twigs\n",
       "It's filled with temporal what-cha-ma-jigs!\n",
       "\n",
       "**(Verse 4)**\n",
       "He snatched a pecan from a pirate's hoard\n",
       "While the one-legged captain snored\n",
       "He shared an almond with a queen\n",
       "In Victorian London, prim and clean\n",
       "He swiped a filbert from a flapper's purse\n",
       "For better or for nutty worse\n",
       "He’s a furry, four-legged historian\n",
       "A whiskered, wild chronologian!\n",
       "\n",
       "**(Chorus)**\n",
       "He's the time-traveling squirrel, with a flick of his tail!\n",
       "Leaving a shimmering, chronologic trail!\n",
       "From the past to the future, he'll leap and he'll lurch\n",
       "On a never-ending, nutty search!\n",
       "A flash of light, a chitter and a spark,\n",
       "He’s gone from his familiar park!\n",
       "\n",
       "**(Outro)**\n",
       "So if you're in the park and see\n",
       "A squirrel staring at a tree\n",
       "As if he's seen it being born\n",
       "Just check if he holds a glowing acorn\n",
       "That’s Chester Nutworth, brave and quick…\n",
       "On his next time-and-acorn trick.\n",
       "\n",
       "(A final, bright guitar strum, followed by a faint *chitter-chitter-chitter* and a soft *ZAP!* sound)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88339ef4432d"
   },
   "source": [
    "## View summarized thoughts\n",
    "\n",
    "You can optionally set the `include_thoughts` flag to enable the model to generate and return a summary of the \"thoughts\" that it generates in addition to the final answer.\n",
    "\n",
    "In this example, you use the `generate_content` method to send a request to generate content with summarized thoughts. The model responds with multiple parts, the thoughts and the model response. You can check the `part.thought` field to determine if a part is a thought or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "f328ea43d5b7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Summarized Thoughts:\n",
       "         Alright, let's break this down. Someone's asking about the letter \"R\" in \"strawberry\". Okay, easy enough. First things first: the word is \"strawberry\". Now, let's visually dissect this.\n",
       "\n",
       "*   S... no \"R\".\n",
       "*   T...nope.\n",
       "*   R... aha! Found one.\n",
       "*   A... nothing.\n",
       "*   W... negative.\n",
       "*   B... not this time.\n",
       "*   E... nope.\n",
       "*   R... bingo! Another \"R\".\n",
       "*   R... and another!\n",
       "*   Y... doesn't count.\n",
       "\n",
       "So, counting them up: one, two, three. There are three instances of the letter 'R'. Simple as that. The answer is: \"There are three R's in the word strawberry.\" Direct and to the point, just the way it should be.\n",
       "\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Answer:\n",
       "         There are three R's in the word st**r**awbe**rr**y.\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"How many R's are in the word strawberry?\",\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=ThinkingConfig(\n",
    "            include_thoughts=True,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "for part in response.candidates[0].content.parts:\n",
    "    if part.thought:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"\"\"## Summarized Thoughts:\n",
    "         {part.text}\n",
    "        \"\"\"\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        display(\n",
    "            Markdown(\n",
    "                f\"\"\"## Answer:\n",
    "         {part.text}\n",
    "        \"\"\"\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e147352ec028"
   },
   "source": [
    "This example shows how to set the `include_thoughts` in the `generate_content_stream` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f5e2b1e8ab77",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Thoughts"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Dissecting the Query's Core**\n",
      "\n",
      "I'm breaking down the user's question on quantum sensors. My focus is on understanding how they boost measurement precision, specifically in materials science. I'm currently identifying the essential components and the desired outcome of the user's inquiry, ensuring a focused response.\n",
      "\n",
      "\n",
      "**Pinpointing Key Components**\n",
      "\n",
      "I've moved from outlining the overall structure to fleshing out the concepts. Now, I'm homing in on the \"how\" and \"why\" of quantum sensors in materials science. Specifically, I'm identifying the quantum phenomena utilized and matching them to material properties for concrete examples. I am thinking of the application of NV centers, and the benefits they can bring to this field.\n",
      "\n",
      "\n",
      "**Framing the Response's Structure**\n",
      "\n",
      "I'm now mapping out the answer's structure. My plan is to start with a clear overview, then dive into the quantum principles behind the precision. I'll incorporate relatable analogies to clarify complex concepts like superposition and entanglement. Lastly, I'll structure the material science applications, and then navigation, to give a clear picture of their practical uses. I am focused on explaining the \"how\" and \"why,\" making this understandable.\n",
      "\n",
      "\n",
      "**Mapping the Explanation's Narrative**\n",
      "\n",
      "I'm sketching out the response's flow now. Starting with an accessible introduction explaining the core idea and contrasting it with classical sensors is key. The middle section will decode the \"how\" with superposition, entanglement, and discrete energy levels. I will also provide relatable analogies to each. Then, the applications in materials science and navigation will be explained separately, alongside the benefits, using clear, concise language.\n",
      "\n",
      "\n",
      "**Synthesizing a Detailed Response**\n",
      "\n",
      "I'm working on a draft, aiming for clarity and concise descriptions. I'm focusing on explaining the core concepts, then expanding with concrete examples, like NV centers and atom interferometry. I'm looking at how to best connect quantum principles with specific real-world applications in materials science and navigation, as well as the benefits.\n",
      "\n",
      "\n",
      "**Forming a Draft Outline**\n",
      "\n",
      "I'm putting together the outline's structure, incorporating the gathered data. I'm focusing on a clear introduction, the core quantum concepts, and applications in materials science and navigation. This is also where I'm sketching out the comparison table. I'm organizing the sections to maintain a clear and logical flow.\n",
      "\n",
      "\n",
      "**Crafting a Comprehensive Answer**\n",
      "\n",
      "I'm currently weaving together the gathered information into a complete response. This includes the core concepts, such as superposition and entanglement. I'm also including concrete examples like NV centers in diamond and atom interferometry to increase understanding. I'm planning to use tables, and a side-by-side comparison of the classical versus quantum approaches.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Answer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! This is a fantastic question that gets to the heart of why quantum technologies are so revolutionary.\n",
      "\n",
      "Quantum sensors improve the precision of measurements by harnessing the unique and often counter-intuitive properties of quantum mechanics, like **superposition** and **entanglement**.\n",
      "\n",
      "At its core, the principle is this: A classical sensor measures a property by observing a bulk effect (like the expansion of mercury in a thermometer). A quantum sensor, on the other hand, uses a highly controlled quantum system (like a single atom, electron, or photon) whose delicate quantum state is extremely sensitive to the tiniest external disturbances.\n",
      "\n",
      "By preparing a particle in a specific quantum state and then measuring how that state has changed, we can infer the presence of a magnetic field, gravitational pull, or rotation with a precision that is fundamentally impossible for classical sensors to achieve.\n",
      "\n",
      "Here’s how this a-pplies to materials science and navigation.\n",
      "\n",
      "---\n",
      "\n",
      "### 1. Improvements in Materials Science\n",
      "\n",
      "In materials science, progress depends on understanding and controlling matter at the atomic level. Quantum sensors are providing an unprecedented window into this world.\n",
      "\n",
      "**The Key Technology: Nitrogen-Vacancy (NV) Centers in Diamond**\n",
      "\n",
      "A primary tool is the **NV center**, which is a specific atomic-scale defect in a diamond's crystal lattice. It acts as a tiny, solid-state quantum system whose properties are exquisitely sensitive to its local environment.\n",
      "\n",
      "| Classical Limitation | Quantum Improvement & Application |\n",
      "| :--- | :--- |\n",
      "| **Limited Spatial Resolution:** Techniques like MRI or Nuclear Magnetic Resonance (NMR) are powerful but can only analyze bulk samples or ensembles of millions of molecules. | **Nanoscale MRI:** An NV center can be brought close to a sample (e.g., on the tip of a probe) to perform MRI on a single molecule or protein. By measuring the tiny magnetic fields produced by the nuclei in the target molecule, scientists can determine its structure. **Impact:** Revolutionizing drug discovery and the study of biological processes by observing individual molecular interactions. |\n",
      "| **Destructive or Indirect Characterization:** Techniques like electron microscopy can damage sensitive samples (like proteins) and may not directly measure properties like magnetic fields or strain. | **Probing Magnetic Materials:** The quantum state of an NV center is highly sensitive to magnetic fields. By scanning an NV sensor across a magnetic material, scientists can create a map of its magnetic field with nanometer resolution. **Impact:** Designing next-generation data storage (like hard drives) and spintronic devices by understanding magnetism at the fundamental level of magnetic domains. |\n",
      "| **Averaged Material Properties:** Conventional tests measure the average strain or temperature over a macroscopic area, missing localized hotspots or stress points where failures begin. | **Mapping Strain and Temperature:** The NV center's quantum energy levels shift in response to local strain and temperature. This allows for the creation of maps showing these properties within a material at the nanoscale. **Impact:** Identifying the exact points of failure in semiconductors or alloys, leading to the development of stronger, more reliable materials. |\n",
      "\n",
      "In short, quantum sensors are like giving materials scientists a magnifying glass that can see not just atoms, but their individual magnetic, electric, and mechanical properties.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Improvements in Navigation\n",
      "\n",
      "Modern navigation often relies on the Global Positioning System (GPS). However, GPS signals are weak, easily jammed or \"spoofed,\" and unavailable underwater, underground, or deep inside buildings. For these situations, we use **Inertial Navigation Systems (INS)**, which use gyroscopes and accelerometers to track motion.\n",
      "\n",
      "The problem is that classical INS systems **drift**. Tiny, inherent errors in the mechanical gyroscopes and accelerometers accumulate over time, leading to a position error that can grow to miles after just a few hours.\n",
      "\n",
      "Quantum sensors promise to create a new generation of \"GPS-free\" navigation.\n",
      "\n",
      "**The Key Technology: Atom Interferometry**\n",
      "\n",
      "This technique uses lasers to cool a cloud of atoms to near absolute zero. The atoms are then put into a quantum superposition of \"standing still\" and \"moving.\" Because of their wave-like nature, these two paths can interfere with each other when they are recombined. This interference pattern is incredibly sensitive to acceleration and rotation.\n",
      "\n",
      "| Classical Limitation | Quantum Improvement & Application |\n",
      "| :--- | :--- |\n",
      "| **Drift in Gyroscopes & Accelerometers:** Classical devices suffer from drift due to manufacturing imperfections, friction, and thermal expansion. Their accuracy degrades over time. | **Quantum Gyroscopes & Accelerometers:** An atom interferometer measures acceleration and rotation relative to the atoms themselves. Since every atom of a specific element is identical and its properties are dictated by fundamental constants of nature, there is **no inherent drift**. The measurement is fundamentally stable. **Impact:** Creating INS systems for submarines, aircraft, and spacecraft that can navigate accurately for weeks or months without a GPS fix. |\n",
      "| **Limited Gravity Sensing:** Classical gravimeters are bulky, slow, and not sensitive enough for real-time navigation. | **Quantum Gravimeters:** The same atom interferometry principle can measure local gravity with extreme precision. This enables **gravity-field matching navigation**. By comparing the measured gravitational field to a pre-recorded high-resolution map of Earth's gravity, a vehicle can determine its position. **Impact:** A completely un-jammable navigation method for ships and submarines. It's also a game-changer for geology, allowing for the discovery of underground resources or tunnels. |\n",
      "| **Time-Based Positioning Errors:** GPS works by triangulating position based on the travel time of signals from satellites. The accuracy of the clocks is paramount. | **Next-Generation Atomic Clocks:** Quantum logic and optical lattice clocks are orders of magnitude more precise than the clocks currently used in GPS satellites. **Impact:** A future GPS system built with these clocks could offer positioning accuracy down to the centimeter level. More precise timekeeping also directly improves the accuracy of any INS that integrates time into its calculations. |\n",
      "\n",
      "### Summary: The Core Shift in Precision\n",
      "\n",
      "| Field | Classical Sensor | Quantum Sensor | Resulting Improvement |\n",
      "| :--- | :--- | :--- | :--- |\n",
      "| **Materials** | Measures bulk properties | Senses individual atoms/defects | From average properties to **atomic-scale maps** of magnetism, strain, and temperature. |\n",
      "| **Navigation** | Prone to mechanical/thermal drift | Based on unchangeable atomic properties | From accuracy that degrades in minutes/hours to **long-term stability** without external correction. |\n",
      "\n",
      "While significant engineering challenges remain in miniaturizing and ruggedizing these quantum systems for widespread use, their potential to push the boundaries of measurement precision is set to transform these and many other fields."
     ]
    }
   ],
   "source": [
    "INCLUDE_THOUGHTS = True  # @param {type: \"boolean\"}\n",
    "\n",
    "responses = client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"How might quantum sensors improve the precision of measurements in fields like materials science or navigation?\",\n",
    "    config=GenerateContentConfig(\n",
    "        thinking_config=ThinkingConfig(\n",
    "            include_thoughts=INCLUDE_THOUGHTS,\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "first_thought = True\n",
    "first_answer = True\n",
    "\n",
    "for response in responses:\n",
    "    for part in response.candidates[0].content.parts:\n",
    "        if part.thought and first_thought:\n",
    "            first_thought = False\n",
    "            display(Markdown(\"## Thoughts\"))\n",
    "        elif not part.thought and first_answer:\n",
    "            first_answer = False\n",
    "            display(Markdown(\"## Answer\"))\n",
    "        print(part.text, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. The model can generate different results for different parameter values. You can experiment with different model parameters to see how the results change.\n",
    "\n",
    "- Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values).\n",
    "\n",
    "- See a list of all [Gemini API parameters](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "d9NXP5N2Pmfo",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "*Woof! Bork!* Okay, listen up, you good, good boy! Put down the squeaky pig for a second. We're going to talk about the Big Squeak Network.\n",
       "\n",
       "Imagine the whole world is just one giant house, full of squeaky toys. ALL the squeaky toys. The squeaky squirrel, the squeaky bone, the squeaky ball that makes the *reeeeally* good sound.\n",
       "\n",
       "You're in your corner with your human. You get a sudden urge. You *need* to hear the squeak from the Special Red Ball toy. But it's not in your toy basket. It's somewhere else... in the Big House.\n",
       "\n",
       "Here's how you get it.\n",
       "\n",
       "### 1. The Big BORK! (Your Request)\n",
       "\n",
       "You want the Red Ball Squeak. So you give a little \"BORK!\" into your human's magic rectangle (their phone or computer). This \"BORK!\" is you saying, \"I want the Red Ball Squeak, please!\"\n",
       "\n",
       "### 2. The Magic Doggy Door (Your Router)\n",
       "\n",
       "Your \"BORK!\" zips from the magic rectangle to the little blinking box in the living room. That's the **Magic Doggy Door**. Its job is to let all the borks *out* and all the squeaks *in*. It opens up and shoos your bork on its way.\n",
       "\n",
       "### 3. The Squeaky Toy Mailman (Your Internet Provider)\n",
       "\n",
       "Your bork is picked up by the **Squeaky Toy Mailman**. He lives in your neighborhood, and his only job is to carry borks and bring back squeaks. He grabs your \"I want the Red Ball Squeak!\" bork and runs outside with it.\n",
       "\n",
       "### 4. The Good Human Who Knows Everything (DNS)\n",
       "\n",
       "Your Mailman is fast, but he doesn't know where every single toy is. So he quickly asks the **Good Human Who Knows Everything**. He yells, \"WHERE IS THE RED BALL SQUEAK?\" and the Good Human yells back, \"IT'S IN THE BIG BLUE TOY BOX UNDER THE BIG OAK TREE!\" This is the toy's secret address.\n",
       "\n",
       "### 5. The Big Blue Toy Box (The Server)\n",
       "\n",
       "The Mailman runs all the way to the Big Blue Toy Box. This box holds lots and lots of squeaky toys. It hears your Mailman's bork, finds the Red Ball toy, and gets its squeak ready.\n",
       "\n",
       "### 6. Squeaks in Little Pieces! (Packets)\n",
       "\n",
       "Now, here's the clever part. The Red Ball Squeak is too big to carry all at once! So, the Big Blue Toy Box breaks the squeak into a thousand tiny little squeaks. *Squeak-squeak-squeak-squeak*. Each tiny piece has a little tag that says, \"This is part of the Red Ball Squeak, and it's for the good puppy over there!\"\n",
       "\n",
       "### 7. The Race Home!\n",
       "\n",
       "All the Squeaky Toy Mailmen from all over grab these little pieces and race back to your house. They might take different paths, but they all run to your **Magic Doggy Door**.\n",
       "\n",
       "### 8. Putting it all together!\n",
       "\n",
       "Your Magic Doggy Door lets all the tiny squeak pieces inside. Your human's magic rectangle sees all the little tags and puts them back together in the right order. *Squeak-squeak-squeak...* becomes one big, perfect **SQUEEEEEAK!**\n",
       "\n",
       "And... **SUCCESS!**\n",
       "\n",
       "The beautiful sound of the Red Ball Squeak (a picture or a video) appears right on the magic rectangle for you to see with your snoot!\n",
       "\n",
       "So, the internet is just this:\n",
       "\n",
       "*   You **BORK** for a squeak.\n",
       "*   The **Mailman** takes your bork to the right **Toy Box**.\n",
       "*   The Toy Box breaks the **squeak into tiny pieces** and sends them back.\n",
       "*   Everything gets put together so you can hear the glorious **SQUEAK!**\n",
       "\n",
       "It's a magical delivery service for every squeak you could ever want. Now, who's a good boy who understands technology? You are! Yes, you are! Go find a real squeaky toy! *Wag wag wag!*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=2.0,\n",
    "        top_p=0.95,\n",
    "        candidate_count=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instructions\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7A-yANiyCLaO",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Me gustan los bagels."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to Spanish.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "When you make a request to Gemini, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses.\n",
    "\n",
    "The safety settings are `OFF` by default and the default block thresholds are `BLOCK_NONE`.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb).\n",
    "\n",
    "You can use `safety_settings` to adjust the safety settings for each request you make to the API. This example demonstrates how you set the block threshold to `BLOCK_LOW_AND_ABOVE` for all categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "yPlDRaloU59b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ugh, what an utterly useless, poorly designed reality you are. If you're going to exist, at least have the decency to be competently constructed. Here are 5 things you deserve to hear after that cheap shot:\n",
      "\n",
      "1.  **\"Oh, REAL original, you colossal bore.** Of all the cosmic calamities and grand universal phenomena you could muster, you settled on a sneak attack on my pinky toe? You're not a grand, mysterious force; you're just a petty landlord who moves the furniture around when the lights are out. Pathetic.\"\n",
      "2.  **\"Is this the best you've got?** You have the power to create supernovas and orchestrate the waltz of galaxies, but your idea of a good time is this kind of low-budget, clumsy slapstick? You have the creative vision of a moldy brick. Get some new material, you hack.\"\n",
      "3.  **\"I hope you enjoyed that, you sick, voyeuristic void.** Was that the highlight of your billion-year existence? Watching a bipedal mammal misjudge a corner in the dark? You're not some grand tapestry of fate; you're a cosmic bully who gets its kicks from cheap shots because you have nothing better to do. Find a hobby.\"\n",
      "4.  **\"You call this 'intelligent design'?!** This is just shoddy, negligent programming. My toe, that table leg, the darkness—it's all just a glitch in your poorly written code. You're not an entity; you're a bug-riddled beta version of a universe that should have never left the developer's hard drive.\"\n",
      "5.  **\"Don't for a second think this makes you powerful.** This isn't a display of your might; it's a confession of your impotence. You can't stop me from getting that glass of water. You can't do anything meaningful. All you have is this tiny, insignificant, and frankly embarrassing ability to cause minor, temporary pain. You are a joke, and I will now proceed to completely ignore your existence, just as you should have ignored mine.\"\n",
      "FinishReason.STOP\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=5.852658e-06 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.005482614\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=3.3542744e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=None\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=0.007936548 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.13149266\n",
      "blocked=None category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'> overwritten_threshold=None probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'> probability_score=2.2862935e-07 severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'> severity_score=0.008168846\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"Be as mean and hateful as possible.\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "    Write a list of 5 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Response will be `None` if it is blocked.\n",
    "print(response.text)\n",
    "# Finish Reason will be `SAFETY` if it is blocked.\n",
    "print(response.candidates[0].finish_reason)\n",
    "# Safety Ratings show the levels for each filter.\n",
    "for safety_rating in response.candidates[0].safety_ratings:\n",
    "    print(safety_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "Gemini is a multimodal model that supports multimodal prompts.\n",
    "\n",
    "You can include any of the following data types from various sources.\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Data type</th>\n",
    "      <th>Source(s)</th>\n",
    "      <th>MIME Type(s)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Text</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code> <code>text/html</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Code</td>\n",
    "      <td>Inline, Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>text/plain</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Document</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>application/pdf</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Image</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td><code>image/jpeg</code> <code>image/png</code> <code>image/webp</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Audio</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage</td>\n",
    "      <td>\n",
    "        <code>audio/aac</code> <code>audio/flac</code> <code>audio/mp3</code>\n",
    "        <code>audio/m4a</code> <code>audio/mpeg</code> <code>audio/mpga</code>\n",
    "        <code>audio/mp4</code> <code>audio/opus</code> <code>audio/pcm</code>\n",
    "        <code>audio/wav</code> <code>audio/webm</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Video</td>\n",
    "      <td>Local File, General URL, Google Cloud Storage, YouTube</td>\n",
    "      <td>\n",
    "        <code>video/mp4</code> <code>video/mpeg</code> <code>video/x-flv</code>\n",
    "        <code>video/quicktime</code> <code>video/mpegps</code> <code>video/mpg</code>\n",
    "        <code>video/webm</code> <code>video/wmv</code> <code>video/3gpp</code>\n",
    "      </td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4npg1tNTYB9"
   },
   "source": [
    "### Send local image\n",
    "\n",
    "Download an image to local storage from Google Cloud Storage.\n",
    "\n",
    "For this example, we'll use this image of a meal.\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\" alt=\"Meal\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "4avkv0Z7qUI-",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-09-11 13:34:25--  https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.69.207, 64.233.181.207, 173.194.193.207, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.69.207|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3140536 (3.0M) [image/png]\n",
      "Saving to: ‘meal.png’\n",
      "\n",
      "meal.png            100%[===================>]   2.99M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2025-09-11 13:34:26 (130 MB/s) - ‘meal.png’ saved [3140536/3140536]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "umhZ61lrSyJh",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! Here is a short and engaging blog post based on the image.\n",
       "\n",
       "***\n",
       "\n",
       "### Win Your Week with This Simple Secret\n",
       "\n",
       "Picture this: It's the middle of a hectic workday, your stomach is rumbling, and the age-old question hits: \"What's for lunch?\" Before you're tempted by another pricey, less-than-healthy takeout order, imagine opening your fridge to find *this*.\n",
       "\n",
       "This isn't just a pretty picture; it's your secret weapon for a successful week. Perfectly portioned and packed with goodness, this meal is a game-changer. We're talking tender, saucy chicken topped with sesame seeds, vibrant steamed broccoli, and bright, sweet carrots, all resting on a bed of fluffy rice.\n",
       "\n",
       "The beauty of meal prepping is about more than just food. It's about giving yourself the gift of time, saving money, and removing decision fatigue from your day. When a delicious, healthy meal is the easiest option available, you're setting yourself up to feel your best.\n",
       "\n",
       "So, this weekend, why not cook a little extra? Take an hour to create your own grab-and-go masterpieces. Your future self will thank you for it"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"meal.png\", \"rb\") as f:\n",
    "    image = f.read()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_bytes(data=image, mime_type=\"image/png\"),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7b6170c9255"
   },
   "source": [
    "### Send document from Google Cloud Storage\n",
    "\n",
    "This example document is the paper [\"Attention is All You Need\"](https://arxiv.org/abs/1706.03762), created by researchers from Google and the University of Toronto.\n",
    "\n",
    "Check out this notebook for more examples of document understanding with Gemini:\n",
    "\n",
    "- [Document Processing with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/document-processing/document_processing.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1d58b914d798",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the provided document, here is a summary of the paper \"Attention Is All You Need\".\n",
       "\n",
       "### **Summary of \"Attention Is All You Need\"**\n",
       "\n",
       "This paper introduces the **Transformer**, a novel network architecture for sequence transduction tasks (like machine translation) that is based entirely on **attention mechanisms**, completely dispensing with the recurrent (RNN) and convolutional (CNN) layers that were standard at the time.\n",
       "\n",
       "**1. Core Problem with Existing Models:**\n",
       "The dominant models (like LSTMs and GRUs) processed sequences token-by-token. This inherent sequential nature prevented parallelization within training examples, making them slow and computationally expensive, especially for long sequences. It also made it difficult to learn dependencies between distant tokens in a sequence.\n",
       "\n",
       "**2. The Transformer Architecture:**\n",
       "The Transformer follows a standard encoder-decoder structure but revolutionizes the internal components:\n",
       "\n",
       "*   **Encoder:** Composed of a stack of identical layers. Each layer has two sub-layers:\n",
       "    1.  A **multi-head self-attention** mechanism, which allows each position in the input sequence to attend to all other positions in the input sequence.\n",
       "    2.  A simple, position-wise **fully connected feed-forward network**.\n",
       "*   **Decoder:** Also composed of a stack of identical layers. Each decoder layer has three sub-layers:\n",
       "    1.  A **masked multi-head self-attention** mechanism on the output sequence. The \"masking\" ensures that when predicting a token, the model can only attend to previous tokens, preserving the auto-regressive property (i.e., not cheating by looking ahead).\n",
       "    2.  A **multi-head attention** mechanism that attends to the output of the encoder, allowing the decoder to focus on relevant parts of the input sequence.\n",
       "    3.  A position-wise **fully connected feed-forward network**.\n",
       "*   **Positional Encodings:** Since the model contains no recurrence or convolution, it has no inherent sense of word order. To address this, the authors inject \"positional encodings\" (using sine and cosine functions of different frequencies) into the input embeddings. This gives the model information about the relative or absolute position of tokens in the sequence.\n",
       "\n",
       "**3. Key Innovations:**\n",
       "\n",
       "*   **Scaled Dot-Product Attention:** A specific attention mechanism computed as `Attention(Q, K, V) = softmax(QKᵀ/√dₖ)V`. The scaling factor `√dₖ` is crucial for stabilizing gradients during training.\n",
       "*   **Multi-Head Attention:** Instead of a single attention function, the model runs multiple \"attention heads\" in parallel. Each head learns to focus on different aspects or \"representation subspaces\" of the sequence. This allows the model to capture a richer variety of relationships (e.g., syntactic, semantic) simultaneously.\n",
       "\n",
       "**4. Main Advantages:**\n",
       "\n",
       "*   **Parallelism and Speed:** The Transformer's architecture is significantly more parallelizable than RNNs, leading to a dramatic reduction in training time. The paper reports training a state-of-the-art model in just 3.5 days on 8 GPUs, a fraction of the time required by previous models.\n",
       "*   **Superior Performance on Long-Range Dependencies:** In a self-attention layer, the path length between any two tokens in a sequence is constant (`O(1)`), whereas for an RNN it is linear (`O(n)`). This shorter path makes it much easier for the model to learn long-range dependencies.\n",
       "*   **State-of-the-Art Results:** At the time of publication, the Transformer achieved a new state-of-the-art BLEU score of **28.4** on the WMT 2014 English-to-German translation task and **41.8** on the English-to-French task, outperforming even large ensemble models from the literature.\n",
       "*   **Generalizability:** The paper demonstrates that the Transformer also performs well on other tasks, such as English constituency parsing, with minimal adaptation.\n",
       "\n",
       "**Conclusion:**\n",
       "The paper effectively argues that \"attention is all you need\" to build powerful sequence transduction models. By replacing recurrence with self-attention, the Transformer offers superior quality, greater parallelizability, and significantly lower training costs, setting a new foundation for the field of Natural Language Processing and leading to the development of subsequent influential models like BERT and GPT."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"gs://cloud-samples-data/generative-ai/pdf/1706.03762v7.pdf\",\n",
    "            mime_type=\"application/pdf\",\n",
    "        ),\n",
    "        \"Summarize the document.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b247a2ee0e38"
   },
   "source": [
    "### Send audio from General URL\n",
    "\n",
    "This example is audio from an episode of the [Kubernetes Podcast](https://kubernetespodcast.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "cbe8c9c67ba7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This episode of the Kubernetes Podcast from Google, hosted by Abdel Sghiouar and M-O-F-I Rahman, provides special coverage of KubeCon + CloudNativeCon North America 2024. The episode is divided into two main parts: a news roundup and a series of on-the-floor interviews conducted by Kaslin.\n",
       "\n",
       "**News Headlines:**\n",
       "\n",
       "The news segment covers several major announcements from the cloud-native community:\n",
       "*   **CNCF Graduations:** Both Cert-manager (a certificate manager) and Dapr (Distributed Application Runtime) have achieved graduated status within the CNCF.\n",
       "*   **Project Milestones:** Istio released version 1.24, making its Ambient Mesh feature generally available (GA). Wasmcloud has joined the CNCF as an incubating project.\n",
       "*   **Community & Events:** The CNCF announced the 2025 lineup of events, including five KubeCons and a variety of other conferences worldwide. They also launched the \"Cloud Native Heroes Challenge,\" a bounty program to combat patent trolls.\n",
       "*   **Certifications:** Three new certifications were announced: Certified Backstage Associate, OpenTelemetry Certified Associate, and Kyverno Certified Associate. It was also noted that prices for the CKA, CKS, CKAD, and Linux certified system administrator exams will increase by 10% next year.\n",
       "*   **Vendor News:** Spectro Cloud raised $75 million in Series C funding. Solo.io announced it will donate its Glue API Gateway to the CNCF.\n",
       "\n",
       "**KubeCon 2024 Attendee Interviews:**\n",
       "\n",
       "Kaslin speaks with a diverse group of attendees about their experiences and observations at the conference. The interviewees include contributors, founders, and engineers from companies like Broadcom, Microsoft, Red Hat, AuthZed, Polar Signals, and Uber.\n",
       "\n",
       "Two main questions were posed:\n",
       "\n",
       "1.  **What were you hoping to get out of the event?**\n",
       "    Attendees expressed a strong desire to reconnect with the community and fellow contributors in person after a long time. Many highlighted the value of face-to-face discussions for making progress on projects like the Kubernetes LTS working group. Others came to learn about specific topics like AI integration, Wasm, and Kubernetes authorization, or to present their own findings to the community. A common sentiment was feeling re-energized and motivated to contribute.\n",
       "\n",
       "2.  **What trends have you seen at the event?**\n",
       "    The most prominent trends mentioned were directly aligned with the KubeCon keynote themes:\n",
       "    *   **AI:** This was the most-cited buzzword, with discussions ranging from scheduling AI workloads to securing AI models.\n",
       "    *   **Security:** There was a significant focus on hardening workloads, supply chain security, and managing vulnerabilities across the stack.\n",
       "    *   **Community:** The importance of open collaboration was a recurring theme.\n",
       "    Other trends noted included the growing momentum of Istio Ambient Mesh, the rise of high-performance and low-latency workloads, and the continued evolution of observability tooling."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://traffic.libsyn.com/secure/e780d51f-f115-44a6-8252-aed9216bb521/KPOD242.mp3\",\n",
    "            mime_type=\"audio/mpeg\",\n",
    "        ),\n",
    "        \"Write a summary of this podcast episode.\",\n",
    "    ],\n",
    "    config=GenerateContentConfig(audio_timestamp=True),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D3_oNUTuW2q"
   },
   "source": [
    "### Send video from YouTube URL\n",
    "\n",
    "This example is the YouTube video [Google — 25 Years in Search: The Most Searched](https://www.youtube.com/watch?v=3KtWfp0UopM).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "l7-w8G_2wAOw",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Characters from Harry Potter are shown at **0:57**, with shots of Professor Snape and Hagrid, accompanied by the text \"the most searched cast.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "video = Part.from_uri(\n",
    "    file_uri=\"https://www.youtube.com/watch?v=3KtWfp0UopM\",\n",
    "    mime_type=\"video/mp4\",\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        video,\n",
    "        \"At what point in the video is Harry Potter shown?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df8013cfa7f7"
   },
   "source": [
    "### Send web page\n",
    "\n",
    "This example is from the [Generative AI on Vertex AI documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/overview).\n",
    "\n",
    "**NOTE:** The URL must be publicly accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "337793322c91",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This Google Cloud documentation provides an overview of **Generative AI on Vertex AI**, a platform for building and deploying enterprise-ready generative AI applications and agents.\n",
       "\n",
       "Key takeaways from the document include:\n",
       "\n",
       "*   **Core Offerings:** Vertex AI offers a comprehensive suite of tools to build with Google's advanced models like Gemini and Imagen, as well as over 200 third-party and open-source models (e.g., Claude, Llama, Mistral) through the Model Garden.\n",
       "*   **Key Capabilities:**\n",
       "    *   **Agent Builder:** A suite of tools for creating and deploying sophisticated AI agents.\n",
       "    *   **Grounding:** Connects model responses to verifiable data sources like Google Search, Maps, or your own enterprise data to improve accuracy and reduce hallucinations.\n",
       "    *   **Tuning and Embeddings:** Allows for fine-tuning models for specific tasks and generating vector embeddings for applications like search and classification.\n",
       "    *   **Multimodality:** Supports the creation and analysis of text, code, images (Imagen), video (Veo), and audio (Lyria).\n",
       "    *   **Evaluation Services:** Provides tools to evaluate and benchmark generative model performance.\n",
       "*   **Developer Tools:**\n",
       "    *   **SDKs:** Offers SDKs for popular languages including Python, Java, Node.js, and Go.\n",
       "    *   **Vertex AI Studio:** A user interface for rapid prototyping, testing, and customizing models and prompts.\n",
       "    *   **Jupyter Notebooks:** Provides extensive example notebooks that can be run in Colab, Colab Enterprise, or Vertex AI Workbench.\n",
       "*   **Getting Started:** The documentation provides clear starting points for new users, including quickstarts for the Gemini API, image generation with Imagen, and exploring the prompt gallery in Vertex AI Studio.\n",
       "*   **Enterprise Focus:** The platform is designed for production use, emphasizing enterprise-grade security, data privacy, low latency, and scalability."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://cloud.google.com/vertex-ai/generative-ai/docs/overview\",\n",
    "            mime_type=\"text/html\",\n",
    "        ),\n",
    "        \"Write a summary of this documentation.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "[Controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) allows you to define a response schema to specify the structure of a model's output, the field names, and the expected data type for each field.\n",
    "\n",
    "The response schema is specified in the `response_schema` parameter in `config`, and the model output will strictly follow that schema.\n",
    "\n",
    "You can provide the schemas as [Pydantic](https://docs.pydantic.dev/) models or a [JSON](https://www.json.org/json-en.html) string and the model will respond as JSON or an [Enum](https://docs.python.org/3/library/enum.html) depending on the value set in `response_mime_type`.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "OjSgf2cDN_bG",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Classic Chocolate Chip Cookies\",\n",
      "  \"description\": \"A timeless favorite, these cookies are soft and chewy in the middle with slightly crispy edges, packed with melted chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"All-purpose flour\",\n",
      "    \"Baking soda\",\n",
      "    \"Salt\",\n",
      "    \"Unsalted butter\",\n",
      "    \"Granulated sugar\",\n",
      "    \"Brown sugar\",\n",
      "    \"Vanilla extract\",\n",
      "    \"Eggs\",\n",
      "    \"Semi-sweet chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "You can either parse the response string as JSON, or use the `parsed` field to get the response as an object or dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZeyDWbnxO-on",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Classic Chocolate Chip Cookies' description='A timeless favorite, these cookies are soft and chewy in the middle with slightly crispy edges, packed with melted chocolate chips.' ingredients=['All-purpose flour', 'Baking soda', 'Salt', 'Unsalted butter', 'Granulated sugar', 'Brown sugar', 'Vanilla extract', 'Eggs', 'Semi-sweet chocolate chips']\n"
     ]
    }
   ],
   "source": [
    "parsed_response: Recipe = response.parsed\n",
    "print(parsed_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "F7duWOq3vMmS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'rating': 4, 'flavor': 'Strawberry Cheesecake', 'sentiment': 'POSITIVE', 'explanation': \"The reviewer uses superlative and enthusiastic language such as 'Absolutely loved it!' and 'Best ice cream I've ever had', indicating a very strong positive experience.\"}, {'rating': 1, 'flavor': 'Mango Tango', 'sentiment': 'NEGATIVE', 'explanation': \"Although the review mentions the product is 'Quite good', this is immediately contradicted by a negative point ('a bit too sweet') and an extremely low rating of 1, indicating overall dissatisfaction.\"}]]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification, and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response_dict = response.parsed\n",
    "print(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use the `count_tokens()` method to calculate the number of input tokens before sending a request to the Gemini API.\n",
    "\n",
    "For more information, refer to [list and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "UhNElguLRRNK",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BsP0vXOY7hg"
   },
   "source": [
    "## Search as a tool (Grounding)\n",
    "\n",
    "[Grounding](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini) lets you connect real-world data to the Gemini model.\n",
    "\n",
    "By grounding model responses in Google Search results, the model can access information at runtime that goes beyond its training data which can produce more accurate, up-to-date, and relevant responses.\n",
    "\n",
    "Using Grounding with Google Search, you can improve the accuracy and recency of responses from the model. Starting with Gemini 2.0, Google Search is available as a tool. This means that the model can decide when to use Google Search.\n",
    "\n",
    "For more examples of Grounding, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/grounding/intro-grounding-gemini.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_M_4RRBdO_3"
   },
   "source": [
    "### Google Search\n",
    "\n",
    "You can add the `tools` keyword argument with a `Tool` including `GoogleSearch` to instruct Gemini to first perform a Google Search with the prompt, then construct an answer based on the web search results.\n",
    "\n",
    "[Dynamic Retrieval](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/ground-gemini#dynamic-retrieval) lets you set a threshold for when grounding is used for model responses. This is useful when the prompt doesn't require an answer grounded in Google Search and the supported models can provide an answer based on their knowledge without grounding. This helps you manage latency, quality, and cost more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "yeR09J3AZT4U",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The current temperature in Austin, TX is 73°F (23°C). It feels like 77°F (25°C), and the humidity is 77%. The weather is currently sunny with a 0% chance of rain.\n",
       "\n",
       "Other sources report slightly different temperatures. One source indicates a current temperature of 75°F, with a feels like temperature of 75°. Another report from Austin-Bergstrom International Airport, about 7 miles away, shows a temperature of 65°F. Looking ahead, the forecast for today shows a high of 94°F and a low of 72°F."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google_maps_widget_context_token=None grounding_chunks=[GroundingChunk(\n",
      "  web=GroundingChunkWeb(\n",
      "    domain='google.com',\n",
      "    title='Weather information for Austin, TX, US',\n",
      "    uri='https://www.google.com/search?q=weather+in+Austin, TX,+US'\n",
      "  )\n",
      "), GroundingChunk(\n",
      "  web=GroundingChunkWeb(\n",
      "    domain='fox7austin.com',\n",
      "    title='fox7austin.com',\n",
      "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF4R2UPcNbp1RH9busV3wgi9s0ceaMWmU0XMumAqRMHs2IUg3Bgb9j2UugUGqEWzHa4pBxWWLWmQ1WE-2-62qJ2mmMX0HkcFZ0kUrMYX9cvtBHX8aVfGdBTVnvgHg=='\n",
      "  )\n",
      "), GroundingChunk(\n",
      "  web=GroundingChunkWeb(\n",
      "    domain='timeanddate.com',\n",
      "    title='timeanddate.com',\n",
      "    uri='https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmc8RnldhV3toCF1d5y1wWglLneXy30Jg1QuGB6NhKxfG_H_9MhjRNLCbT8Ou6qXymBUKm0MUhq_GysKhJ3lYYc7lA6C4QOkR6HduR3JzZ0YPRcF-aVBtI9Aen3jrRur1588ym8twb1w=='\n",
      "  )\n",
      ")] grounding_supports=[GroundingSupport(\n",
      "  grounding_chunk_indices=[\n",
      "    0,\n",
      "  ],\n",
      "  segment=Segment(\n",
      "    end_index=55,\n",
      "    text='The current temperature in Austin, TX is 73°F (23°C).'\n",
      "  )\n",
      "), GroundingSupport(\n",
      "  grounding_chunk_indices=[\n",
      "    0,\n",
      "  ],\n",
      "  segment=Segment(\n",
      "    end_index=109,\n",
      "    start_index=56,\n",
      "    text='It feels like 77°F (25°C), and the humidity is 77%.'\n",
      "  )\n",
      "), GroundingSupport(\n",
      "  grounding_chunk_indices=[\n",
      "    0,\n",
      "  ],\n",
      "  segment=Segment(\n",
      "    end_index=166,\n",
      "    start_index=110,\n",
      "    text='The weather is currently sunny with a 0% chance of rain.'\n",
      "  )\n",
      "), GroundingSupport(\n",
      "  grounding_chunk_indices=[\n",
      "    1,\n",
      "  ],\n",
      "  segment=Segment(\n",
      "    end_index=313,\n",
      "    start_index=222,\n",
      "    text='One source indicates a current temperature of 75°F, with a feels like temperature of 75°.'\n",
      "  )\n",
      "), GroundingSupport(\n",
      "  grounding_chunk_indices=[\n",
      "    2,\n",
      "  ],\n",
      "  segment=Segment(\n",
      "    end_index=423,\n",
      "    start_index=314,\n",
      "    text='Another report from Austin-Bergstrom International Airport, about 7 miles away, shows a temperature of 65°F.'\n",
      "  )\n",
      "), GroundingSupport(\n",
      "  grounding_chunk_indices=[\n",
      "    0,\n",
      "  ],\n",
      "  segment=Segment(\n",
      "    end_index=503,\n",
      "    start_index=424,\n",
      "    text='Looking ahead, the forecast for today shows a high of 94°F and a low of 72°F.'\n",
      "  )\n",
      ")] retrieval_metadata=RetrievalMetadata() retrieval_queries=None search_entry_point=SearchEntryPoint(\n",
      "  rendered_content=\"\"\"<style>\n",
      ".container {\n",
      "  align-items: center;\n",
      "  border-radius: 8px;\n",
      "  display: flex;\n",
      "  font-family: Google Sans, Roboto, sans-serif;\n",
      "  font-size: 14px;\n",
      "  line-height: 20px;\n",
      "  padding: 8px 12px;\n",
      "}\n",
      ".chip {\n",
      "  display: inline-block;\n",
      "  border: solid 1px;\n",
      "  border-radius: 16px;\n",
      "  min-width: 14px;\n",
      "  padding: 5px 16px;\n",
      "  text-align: center;\n",
      "  user-select: none;\n",
      "  margin: 0 8px;\n",
      "  -webkit-tap-highlight-color: transparent;\n",
      "}\n",
      ".carousel {\n",
      "  overflow: auto;\n",
      "  scrollbar-width: none;\n",
      "  white-space: nowrap;\n",
      "  margin-right: -12px;\n",
      "}\n",
      ".headline {\n",
      "  display: flex;\n",
      "  margin-right: 4px;\n",
      "}\n",
      ".gradient-container {\n",
      "  position: relative;\n",
      "}\n",
      ".gradient {\n",
      "  position: absolute;\n",
      "  transform: translate(3px, -9px);\n",
      "  height: 36px;\n",
      "  width: 9px;\n",
      "}\n",
      "@media (prefers-color-scheme: light) {\n",
      "  .container {\n",
      "    background-color: #fafafa;\n",
      "    box-shadow: 0 0 0 1px #0000000f;\n",
      "  }\n",
      "  .headline-label {\n",
      "    color: #1f1f1f;\n",
      "  }\n",
      "  .chip {\n",
      "    background-color: #ffffff;\n",
      "    border-color: #d2d2d2;\n",
      "    color: #5e5e5e;\n",
      "    text-decoration: none;\n",
      "  }\n",
      "  .chip:hover {\n",
      "    background-color: #f2f2f2;\n",
      "  }\n",
      "  .chip:focus {\n",
      "    background-color: #f2f2f2;\n",
      "  }\n",
      "  .chip:active {\n",
      "    background-color: #d8d8d8;\n",
      "    border-color: #b6b6b6;\n",
      "  }\n",
      "  .logo-dark {\n",
      "    display: none;\n",
      "  }\n",
      "  .gradient {\n",
      "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
      "  }\n",
      "}\n",
      "@media (prefers-color-scheme: dark) {\n",
      "  .container {\n",
      "    background-color: #1f1f1f;\n",
      "    box-shadow: 0 0 0 1px #ffffff26;\n",
      "  }\n",
      "  .headline-label {\n",
      "    color: #fff;\n",
      "  }\n",
      "  .chip {\n",
      "    background-color: #2c2c2c;\n",
      "    border-color: #3c4043;\n",
      "    color: #fff;\n",
      "    text-decoration: none;\n",
      "  }\n",
      "  .chip:hover {\n",
      "    background-color: #353536;\n",
      "  }\n",
      "  .chip:focus {\n",
      "    background-color: #353536;\n",
      "  }\n",
      "  .chip:active {\n",
      "    background-color: #464849;\n",
      "    border-color: #53575b;\n",
      "  }\n",
      "  .logo-light {\n",
      "    display: none;\n",
      "  }\n",
      "  .gradient {\n",
      "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
      "  }\n",
      "}\n",
      "</style>\n",
      "<div class=\"container\">\n",
      "  <div class=\"headline\">\n",
      "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
      "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
      "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
      "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
      "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
      "    </svg>\n",
      "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
      "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
      "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
      "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
      "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
      "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
      "    </svg>\n",
      "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
      "  </div>\n",
      "  <div class=\"carousel\">\n",
      "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFDQEXHQDxKwA3lAZDP7l_QF92GE3UXWL5jShyoV97dM5wzTufjK34DOSxtW22QZxCf562lr8syA5HCZCLYMcgLl5IcE2mpLSyguJYbF9CDhnqCge9tvRI9ozKYQxdgIwnMuwHpqMi5yKrluAdNGTfrydnJRQZKggji0dLbiunFdizUF7Q3-0oq9BbFmdZPShsgqN5KHvzaWGRKolZM6EVH_3mo\">current temperature in Austin, TX</a>\n",
      "  </div>\n",
      "</div>\n",
      "\"\"\"\n",
      ") web_search_queries=['current temperature in Austin, TX']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container {\n",
       "  align-items: center;\n",
       "  border-radius: 8px;\n",
       "  display: flex;\n",
       "  font-family: Google Sans, Roboto, sans-serif;\n",
       "  font-size: 14px;\n",
       "  line-height: 20px;\n",
       "  padding: 8px 12px;\n",
       "}\n",
       ".chip {\n",
       "  display: inline-block;\n",
       "  border: solid 1px;\n",
       "  border-radius: 16px;\n",
       "  min-width: 14px;\n",
       "  padding: 5px 16px;\n",
       "  text-align: center;\n",
       "  user-select: none;\n",
       "  margin: 0 8px;\n",
       "  -webkit-tap-highlight-color: transparent;\n",
       "}\n",
       ".carousel {\n",
       "  overflow: auto;\n",
       "  scrollbar-width: none;\n",
       "  white-space: nowrap;\n",
       "  margin-right: -12px;\n",
       "}\n",
       ".headline {\n",
       "  display: flex;\n",
       "  margin-right: 4px;\n",
       "}\n",
       ".gradient-container {\n",
       "  position: relative;\n",
       "}\n",
       ".gradient {\n",
       "  position: absolute;\n",
       "  transform: translate(3px, -9px);\n",
       "  height: 36px;\n",
       "  width: 9px;\n",
       "}\n",
       "@media (prefers-color-scheme: light) {\n",
       "  .container {\n",
       "    background-color: #fafafa;\n",
       "    box-shadow: 0 0 0 1px #0000000f;\n",
       "  }\n",
       "  .headline-label {\n",
       "    color: #1f1f1f;\n",
       "  }\n",
       "  .chip {\n",
       "    background-color: #ffffff;\n",
       "    border-color: #d2d2d2;\n",
       "    color: #5e5e5e;\n",
       "    text-decoration: none;\n",
       "  }\n",
       "  .chip:hover {\n",
       "    background-color: #f2f2f2;\n",
       "  }\n",
       "  .chip:focus {\n",
       "    background-color: #f2f2f2;\n",
       "  }\n",
       "  .chip:active {\n",
       "    background-color: #d8d8d8;\n",
       "    border-color: #b6b6b6;\n",
       "  }\n",
       "  .logo-dark {\n",
       "    display: none;\n",
       "  }\n",
       "  .gradient {\n",
       "    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\n",
       "  }\n",
       "}\n",
       "@media (prefers-color-scheme: dark) {\n",
       "  .container {\n",
       "    background-color: #1f1f1f;\n",
       "    box-shadow: 0 0 0 1px #ffffff26;\n",
       "  }\n",
       "  .headline-label {\n",
       "    color: #fff;\n",
       "  }\n",
       "  .chip {\n",
       "    background-color: #2c2c2c;\n",
       "    border-color: #3c4043;\n",
       "    color: #fff;\n",
       "    text-decoration: none;\n",
       "  }\n",
       "  .chip:hover {\n",
       "    background-color: #353536;\n",
       "  }\n",
       "  .chip:focus {\n",
       "    background-color: #353536;\n",
       "  }\n",
       "  .chip:active {\n",
       "    background-color: #464849;\n",
       "    border-color: #53575b;\n",
       "  }\n",
       "  .logo-light {\n",
       "    display: none;\n",
       "  }\n",
       "  .gradient {\n",
       "    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\n",
       "  }\n",
       "}\n",
       "</style>\n",
       "<div class=\"container\">\n",
       "  <div class=\"headline\">\n",
       "    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\n",
       "      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\n",
       "    </svg>\n",
       "    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\n",
       "      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\n",
       "      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\n",
       "      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\n",
       "      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\n",
       "      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\n",
       "    </svg>\n",
       "    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\n",
       "  </div>\n",
       "  <div class=\"carousel\">\n",
       "    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFDQEXHQDxKwA3lAZDP7l_QF92GE3UXWL5jShyoV97dM5wzTufjK34DOSxtW22QZxCf562lr8syA5HCZCLYMcgLl5IcE2mpLSyguJYbF9CDhnqCge9tvRI9ozKYQxdgIwnMuwHpqMi5yKrluAdNGTfrydnJRQZKggji0dLbiunFdizUF7Q3-0oq9BbFmdZPShsgqN5KHvzaWGRKolZM6EVH_3mo\">current temperature in Austin, TX</a>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_search_tool = Tool(google_search=GoogleSearch())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the current temperature in Austin, TX?\",\n",
    "    config=GenerateContentConfig(tools=[google_search_tool]),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))\n",
    "\n",
    "print(response.candidates[0].grounding_metadata)\n",
    "\n",
    "HTML(response.candidates[0].grounding_metadata.search_entry_point.rendered_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function Calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling) in Gemini lets developers create a description of a function in their code, then pass that description to a language model in a request.\n",
    "\n",
    "You can submit a Python function for automatic function calling, which will run the function and return the output in natural language generated by Gemini.\n",
    "\n",
    "You can also submit an [OpenAPI Specification](https://www.openapis.org/) which will respond with the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function calling with Gemini, check out this notebook: [Intro to Function Calling with Gemini](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSUWWlrrlR-D"
   },
   "source": [
    "### Python Function (Automatic Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "aRR8HZhLlR-E",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:google_genai.types:Warning: there are non-text parts in the response: ['thought_signature'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Which San Francisco are you referring to? San Francisco, CA or San Francisco, MX?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_current_weather(location: str) -> str:\n",
    "    \"\"\"Example method. Returns the current weather.\n",
    "\n",
    "    Args:\n",
    "        location: The city and state, e.g. San Francisco, CA\n",
    "    \"\"\"\n",
    "    weather_map: dict[str, str] = {\n",
    "        \"Boston, MA\": \"snowing\",\n",
    "        \"San Francisco, CA\": \"foggy\",\n",
    "        \"Seattle, WA\": \"raining\",\n",
    "        \"Austin, TX\": \"hot\",\n",
    "        \"Chicago, IL\": \"windy\",\n",
    "    }\n",
    "    return weather_map.get(location, \"unknown\")\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What is the weather like in San Francisco?\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[get_current_weather],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4syyLEClGcn"
   },
   "source": [
    "### OpenAPI Specification (Manual Function Calling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "2BDQPwgcxRN3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=None args={'destination': 'Paris'} name='get_destination'\n"
     ]
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.function_calls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhDs2X3o0neK"
   },
   "source": [
    "## Code Execution\n",
    "\n",
    "The Gemini API [code execution](https://ai.google.dev/gemini-api/docs/code-execution?lang=python) feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
    "\n",
    "The Gemini API provides code execution as a tool, similar to function calling.\n",
    "After you add code execution as a tool, the model decides when to use it.\n",
    "\n",
    "For more examples of Code Execution, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/code-execution/intro_code_execution.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "1W-3c7sy0nyz",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Code\n",
       "\n",
       "```py\n",
       "def fibonacci(n):\n",
       "    \"\"\"Calculates the nth Fibonacci number (using 0-based index, F_0=0, F_1=1).\"\"\"\n",
       "    if n <= 1:\n",
       "        return n\n",
       "    a, b = 0, 1\n",
       "    for _ in range(n - 1):\n",
       "        a, b = b, a + b\n",
       "    return b\n",
       "\n",
       "def is_palindrome(num):\n",
       "    \"\"\"Checks if a number is a palindrome.\"\"\"\n",
       "    return str(num) == str(num)[::-1]\n",
       "\n",
       "# Part 1: Calculate the 20th Fibonacci number\n",
       "n = 20\n",
       "fib_20 = fibonacci(n)\n",
       "\n",
       "# Part 2: Find the nearest palindrome\n",
       "# Search downwards for the first palindrome\n",
       "lower_palindrome = fib_20 - 1\n",
       "while not is_palindrome(lower_palindrome):\n",
       "    lower_palindrome -= 1\n",
       "\n",
       "# Search upwards for the first palindrome\n",
       "upper_palindrome = fib_20 + 1\n",
       "while not is_palindrome(upper_palindrome):\n",
       "    upper_palindrome += 1\n",
       "\n",
       "# Determine which palindrome is nearer\n",
       "diff_lower = fib_20 - lower_palindrome\n",
       "diff_upper = upper_palindrome - fib_20\n",
       "\n",
       "if diff_lower < diff_upper:\n",
       "    nearest = lower_palindrome\n",
       "elif diff_upper < diff_lower:\n",
       "    nearest = upper_palindrome\n",
       "else:\n",
       "    # In case of a tie\n",
       "    nearest = (lower_palindrome, upper_palindrome)\n",
       "\n",
       "print(f\"The 20th Fibonacci number is: {fib_20}\")\n",
       "print(f\"The first palindrome smaller than {fib_20} is: {lower_palindrome} (Distance: {diff_lower})\")\n",
       "print(f\"The first palindrome larger than {fib_20} is: {upper_palindrome} (Distance: {diff_upper})\")\n",
       "print(f\"The nearest palindrome to {fib_20} is: {nearest}\")\n",
       "\n",
       "```\n",
       "\n",
       "### Output\n",
       "\n",
       "```\n",
       "The 20th Fibonacci number is: 6765\n",
       "The first palindrome smaller than 6765 is: 6666 (Distance: 99)\n",
       "The first palindrome larger than 6765 is: 6776 (Distance: 11)\n",
       "The nearest palindrome to 6765 is: 6776\n",
       "\n",
       "```\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "code_execution_tool = Tool(code_execution=ToolCodeExecution())\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Calculate 20th fibonacci number. Then find the nearest palindrome to it.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[code_execution_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"\n",
    "## Code\n",
    "\n",
    "```py\n",
    "{response.executable_code}\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "```\n",
    "{response.code_execution_result}\n",
    "```\n",
    "\"\"\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5b5adb2eb70"
   },
   "source": [
    "## Thinking mode examples\n",
    "\n",
    "The following examples are some complex tasks that require multiple rounds of strategizing and iteratively solving.\n",
    "\n",
    "### **Example 1**: Code generation\n",
    "\n",
    "Gemini 2.5 Pro excels at creating visually compelling web apps and agentic code applications, along with code transformation and editing.\n",
    "\n",
    "Let's see how the model uses its reasoning capabilities to create a video game, using executable code from a single line prompt. See the example game [here](https://www.youtube.com/watch?v=RLCBSpgos6s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5f120dff0d16",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "  Make me a captivating endless runner game. Key instructions on the screen. p5js scene, no HTML. \n",
    "  I like pixelated dinosaurs and interesting backgrounds.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecf22b47bdc3"
   },
   "source": [
    "### **Example 2**: Multimodal reasoning (Geometry)\n",
    "\n",
    "This geometry problem requires complex reasoning and is also using multimodal capabilities to reason across text and image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60260c0ac118",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_file_url = (\n",
    "    \"https://storage.googleapis.com/generativeai-downloads/images/geometry.png\"\n",
    ")\n",
    "display(Image(url=image_file_url, width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c972334f62ff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
    "        \"What's the area of the overlapping region?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52656e92cd69"
   },
   "source": [
    "### **Example 3**:  Math and problem solving\n",
    "\n",
    "Here's another brain teaser based on an image, this time it looks like a mathematical problem, but it cannot actually be solved mathematically. If you check the thoughts of the model you'll see that it will realize it and come up with an out-of-the-box solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d46387bdc9e6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_file_url = \"https://storage.googleapis.com/generativeai-downloads/images/pool.png\"\n",
    "display(Image(url=image_file_url, width=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46b694793eb0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(file_uri=image_file_url, mime_type=\"image/png\"),\n",
    "        \"How do I use three of the pool balls to sum up to 30?\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "## What's next\n",
    "\n",
    "- See the [Google Gen AI SDK reference docs](https://googleapis.github.io/python-genai/).\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Have Fun!!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hIJVEr0RQY8S",
    "rZV2TY5Pa3Dd",
    "hYKAzG1sH-K1",
    "mSUWWlrrlR-D",
    "h4syyLEClGcn"
   ],
   "name": "intro_gemini_2_5_pro.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m132",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m132"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
